{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第7期</td>\n",
       "      <td>https://www.sac.net.cn/tzgg/201910/t20191029_1...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第6期</td>\n",
       "      <td>https://www.sac.net.cn/tzgg/201909/t20190912_1...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第5期</td>\n",
       "      <td>https://www.sac.net.cn/tzgg/201908/t20190812_1...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>药品质量抽检不合格名单</td>\n",
       "      <td>国家药品管理局</td>\n",
       "      <td>国家药品管理局</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://so.kaipuyun.cn/s?q=1&amp;qt=%E5%90%8D%E5%8D...</td>\n",
       "      <td>medicine_blacklist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>首批房产中介黑名单</td>\n",
       "      <td>25省市房地产中介协会</td>\n",
       "      <td>深圳房产信息网</td>\n",
       "      <td>20200528</td>\n",
       "      <td>http://news.szhome.com/338103.html</td>\n",
       "      <td>无法识别图片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>第二批批房产中介黑名单</td>\n",
       "      <td>27省市房地产中介协会</td>\n",
       "      <td>网易新闻</td>\n",
       "      <td>20200714</td>\n",
       "      <td>https://3g.163.com/house/article/FHL73QCA00078...</td>\n",
       "      <td>无法识别图片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>首次发布环评信用平台失信黑名单</td>\n",
       "      <td>生态环境部</td>\n",
       "      <td>新华信用</td>\n",
       "      <td>20200708</td>\n",
       "      <td>https://m.credit100.com/xhxy/c/2020-07-21/6245...</td>\n",
       "      <td>environment_blacklist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>全国人力资源诚信服务示范机构候选名单</td>\n",
       "      <td>人力资源和社会保障部</td>\n",
       "      <td>诚信苏州公众号</td>\n",
       "      <td>20200707</td>\n",
       "      <td>https://mp.weixin.qq.com/s/ODgW4cUU6QZvngW8VpIktQ</td>\n",
       "      <td>rlzycxfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>银行涉企违规收费名单</td>\n",
       "      <td>中国银行保险监督管理委员会</td>\n",
       "      <td>中国银保监会公众号</td>\n",
       "      <td>2020第6期</td>\n",
       "      <td>https://mp.weixin.qq.com/s/qNsQmC4fN9S1mbsAA3OdiQ</td>\n",
       "      <td>YHWGSQSFAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>重大违法违规股东名单</td>\n",
       "      <td>中国银行保险监督管理委员会</td>\n",
       "      <td>中国银保监会公众号</td>\n",
       "      <td>20200704</td>\n",
       "      <td>https://mp.weixin.qq.com/s/VMr3X1g_psMkcze_6Vlj6Q</td>\n",
       "      <td>zdwfwggdmd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>非法从事场外配资平台名单</td>\n",
       "      <td>中国证券监督管理委员会</td>\n",
       "      <td>证监会发布公众号</td>\n",
       "      <td>20200708</td>\n",
       "      <td>https://mp.weixin.qq.com/s/lXA6wFetZsMzWPuWQ1JPvA</td>\n",
       "      <td>cwpzpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019年度安全诚信航运公司候选名单</td>\n",
       "      <td>中华人民共和国海事局</td>\n",
       "      <td>中华人民共和国海事局</td>\n",
       "      <td>20200729</td>\n",
       "      <td>https://www.msa.gov.cn/page/article.do?article...</td>\n",
       "      <td>aqcxhkgshxmd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2020年第2批</td>\n",
       "      <td>http://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaix...</td>\n",
       "      <td>NMG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>公开发行股票配售对象黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第1期</td>\n",
       "      <td>https://www.sac.net.cn/tzgg/202007/t20200729_1...</td>\n",
       "      <td>stock_peishou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第4期</td>\n",
       "      <td>http://www.csrc.gov.cn/pub/hainan/xxfw/hnsmjzz...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第3期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/hmd/202003/t202003...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第2期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/hmd/202002/t202002...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2020年第1期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/hmd/202001/t202001...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第8期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201912/t20191...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第7期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201910/t20191...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第6期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/hmd/201909/t201909...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第5期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/hmd/201908/t201908...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第4期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201907/t20190...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第3期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201906/t20190...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第2期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201905/t20190...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>2019年第1期</td>\n",
       "      <td>https://www.sac.net.cn/wlzf/zfjl/201904/t20190...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>广西监管局</td>\n",
       "      <td>2018年第6期</td>\n",
       "      <td>http://www.csrc.gov.cn/guangxi/xxfw/tjyg/20181...</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>非法仿冒券商和投资咨询机构黑名单</td>\n",
       "      <td>中国证券业协会</td>\n",
       "      <td>阿牛直播</td>\n",
       "      <td>2017年第5期</td>\n",
       "      <td>http://zjt.aniu.tv/Index_detail_wzid_107891.shtml</td>\n",
       "      <td>bad_website</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2020年第1批</td>\n",
       "      <td>http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...</td>\n",
       "      <td>NMG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2019年第4批</td>\n",
       "      <td>http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...</td>\n",
       "      <td>无法识别图片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2019年第3批</td>\n",
       "      <td>http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...</td>\n",
       "      <td>无法识别图片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2019年第2批</td>\n",
       "      <td>http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...</td>\n",
       "      <td>无法识别图片</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>拖欠农民工工资“黑名单”</td>\n",
       "      <td>人社部</td>\n",
       "      <td>人社部劳动保障监察局</td>\n",
       "      <td>2019年第1批</td>\n",
       "      <td>http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...</td>\n",
       "      <td>NMG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>全国旅游市场黑名单</td>\n",
       "      <td>文化和旅游部</td>\n",
       "      <td>文化和旅游部</td>\n",
       "      <td>2019/11/27</td>\n",
       "      <td>http://zwgk.mct.gov.cn/auto255/201911/t2019112...</td>\n",
       "      <td>travel_blacklist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0              1           2           3  \\\n",
       "0     非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第7期   \n",
       "1     非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第6期   \n",
       "2     非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第5期   \n",
       "3          药品质量抽检不合格名单        国家药品管理局     国家药品管理局         NaN   \n",
       "4            首批房产中介黑名单    25省市房地产中介协会     深圳房产信息网    20200528   \n",
       "5          第二批批房产中介黑名单    27省市房地产中介协会        网易新闻    20200714   \n",
       "6      首次发布环评信用平台失信黑名单          生态环境部        新华信用    20200708   \n",
       "7   全国人力资源诚信服务示范机构候选名单     人力资源和社会保障部     诚信苏州公众号    20200707   \n",
       "8           银行涉企违规收费名单  中国银行保险监督管理委员会   中国银保监会公众号     2020第6期   \n",
       "9           重大违法违规股东名单  中国银行保险监督管理委员会   中国银保监会公众号    20200704   \n",
       "10        非法从事场外配资平台名单    中国证券监督管理委员会    证监会发布公众号    20200708   \n",
       "11  2019年度安全诚信航运公司候选名单     中华人民共和国海事局  中华人民共和国海事局    20200729   \n",
       "12        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2020年第2批   \n",
       "13      公开发行股票配售对象黑名单        中国证券业协会     中国证券业协会     2020年第1期   \n",
       "14    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第4期   \n",
       "15    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第3期   \n",
       "16    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第2期   \n",
       "17    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2020年第1期   \n",
       "18    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第8期   \n",
       "19    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第7期   \n",
       "20    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第6期   \n",
       "21    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第5期   \n",
       "22    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第4期   \n",
       "23    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第3期   \n",
       "24    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第2期   \n",
       "25    非法仿冒券商和投资咨询机构黑名单        中国证券业协会     中国证券业协会    2019年第1期   \n",
       "26    非法仿冒券商和投资咨询机构黑名单        中国证券业协会       广西监管局    2018年第6期   \n",
       "27    非法仿冒券商和投资咨询机构黑名单        中国证券业协会        阿牛直播    2017年第5期   \n",
       "28        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2020年第1批   \n",
       "29        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2019年第4批   \n",
       "30        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2019年第3批   \n",
       "31        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2019年第2批   \n",
       "32        拖欠农民工工资“黑名单”            人社部  人社部劳动保障监察局    2019年第1批   \n",
       "33           全国旅游市场黑名单         文化和旅游部      文化和旅游部  2019/11/27   \n",
       "\n",
       "                                                    4                      5  \n",
       "0   https://www.sac.net.cn/tzgg/201910/t20191029_1...            bad_website  \n",
       "1   https://www.sac.net.cn/tzgg/201909/t20190912_1...            bad_website  \n",
       "2   https://www.sac.net.cn/tzgg/201908/t20190812_1...            bad_website  \n",
       "3   http://so.kaipuyun.cn/s?q=1&qt=%E5%90%8D%E5%8D...     medicine_blacklist  \n",
       "4                  http://news.szhome.com/338103.html                 无法识别图片  \n",
       "5   https://3g.163.com/house/article/FHL73QCA00078...                 无法识别图片  \n",
       "6   https://m.credit100.com/xhxy/c/2020-07-21/6245...  environment_blacklist  \n",
       "7   https://mp.weixin.qq.com/s/ODgW4cUU6QZvngW8VpIktQ               rlzycxfw  \n",
       "8   https://mp.weixin.qq.com/s/qNsQmC4fN9S1mbsAA3OdiQ             YHWGSQSFAL  \n",
       "9   https://mp.weixin.qq.com/s/VMr3X1g_psMkcze_6Vlj6Q             zdwfwggdmd  \n",
       "10  https://mp.weixin.qq.com/s/lXA6wFetZsMzWPuWQ1JPvA                 cwpzpt  \n",
       "11  https://www.msa.gov.cn/page/article.do?article...           aqcxhkgshxmd  \n",
       "12  http://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaix...                    NMG  \n",
       "13  https://www.sac.net.cn/tzgg/202007/t20200729_1...          stock_peishou  \n",
       "14  http://www.csrc.gov.cn/pub/hainan/xxfw/hnsmjzz...            bad_website  \n",
       "15  https://www.sac.net.cn/wlzf/hmd/202003/t202003...            bad_website  \n",
       "16  https://www.sac.net.cn/wlzf/hmd/202002/t202002...            bad_website  \n",
       "17  https://www.sac.net.cn/wlzf/hmd/202001/t202001...            bad_website  \n",
       "18  https://www.sac.net.cn/wlzf/zfjl/201912/t20191...            bad_website  \n",
       "19  https://www.sac.net.cn/wlzf/zfjl/201910/t20191...            bad_website  \n",
       "20  https://www.sac.net.cn/wlzf/hmd/201909/t201909...            bad_website  \n",
       "21  https://www.sac.net.cn/wlzf/hmd/201908/t201908...            bad_website  \n",
       "22  https://www.sac.net.cn/wlzf/zfjl/201907/t20190...            bad_website  \n",
       "23  https://www.sac.net.cn/wlzf/zfjl/201906/t20190...            bad_website  \n",
       "24  https://www.sac.net.cn/wlzf/zfjl/201905/t20190...            bad_website  \n",
       "25  https://www.sac.net.cn/wlzf/zfjl/201904/t20190...            bad_website  \n",
       "26  http://www.csrc.gov.cn/guangxi/xxfw/tjyg/20181...            bad_website  \n",
       "27  http://zjt.aniu.tv/Index_detail_wzid_107891.shtml            bad_website  \n",
       "28  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...                    NMG  \n",
       "29  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...                 无法识别图片  \n",
       "30  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...                 无法识别图片  \n",
       "31  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...                 无法识别图片  \n",
       "32  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodon...                    NMG  \n",
       "33  http://zwgk.mct.gov.cn/auto255/201911/t2019112...       travel_blacklist  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -r requirements.txt\n",
    "\n",
    "import xlrd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import pdfplumber\n",
    "import re\n",
    "from lxml import etree\n",
    "import cx_Oracle as oracle\n",
    "import re\n",
    "import docx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "from selenium import webdriver\n",
    "import socket\n",
    "import time\n",
    "import ssl\n",
    "df = pd.read_csv(\"excel_urls.csv\",header=None,delimiter=\"\\t\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "24\n",
      "Skipping this, need special attention for https://www.sac.net.cn/wlzf/zfjl/201905/t20190507_138578.html\n",
      "非法仿冒券商和投资咨询机构黑名单\n",
      "23\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "22\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "21\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "20\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "19\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "18\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "17\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "16\n",
      "File already exsits!\n",
      "File size = 0.03 Mb\n",
      "15\n",
      "File already exsits!\n",
      "File size = 0.05 Mb\n",
      "14\n",
      "File already exsits!\n",
      "File size = 0.05 Mb\n",
      "13\n",
      "File already exsits!\n",
      "File size = 0.03 Mb\n",
      "0\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "1\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n",
      "2\n",
      "File already exsits!\n",
      "File size = 0.04 Mb\n"
     ]
    }
   ],
   "source": [
    "#如果需要下载docx文件，请运行该cell\n",
    "#如果已经有所有的docx文件，不需要运行这个cell\n",
    "def get_driver_url_content(url, encoding='utf-8', timeout=3):\n",
    "    '''\n",
    "    使用浏览器获取动态内容\n",
    "    :param url:         网页url\n",
    "    :param encoding:    网页编码\n",
    "    :param timeout:     设置超时\n",
    "    :return:\n",
    "    '''\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    # 也可以使用phantomJS\n",
    "    # driver =webdriver.Phantomjs(executable_path=\"/path/to/phantomjs\")\n",
    "    driver.get(url)\n",
    "    time.sleep(timeout)\n",
    "    try:\n",
    "        files = driver.find_elements_by_xpath(\"//*[contains(text(), 'docx')]\")\n",
    "        for i in files:\n",
    "            if i.text!=\"\":\n",
    "                file = i\n",
    "    \n",
    "        href = file.get_attribute(\"href\")\n",
    "        driver.close()\n",
    "        \n",
    "    except:\n",
    "        driver.close()\n",
    "    return href    \n",
    "\n",
    "\n",
    "\n",
    "def download(url,name,savepath='./'):\n",
    "    \"\"\"\n",
    "    download file from internet\n",
    "    :param url: path to download from\n",
    "    :param savepath: path to save files\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    def reporthook(a, b, c):\n",
    "        \"\"\"\n",
    "        显示下载进度\n",
    "        :param a: 已经下载的数据块\n",
    "        :param b: 数据块的大小\n",
    "        :param c: 远程文件大小\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"\\rdownloading: %5.1f%%\" % (a * b * 100.0 / c), end=\"\")\n",
    "    # 判断文件是否存在，如果不存在则下载\n",
    "    filename = name\n",
    "    if not os.path.isfile(os.path.join(savepath, filename)):\n",
    "        print('Downloading data from %s' % url)\n",
    "        urlretrieve(url, os.path.join(savepath, filename), reporthook=reporthook)\n",
    "        print('\\nDownload finished!')\n",
    "    else:\n",
    "        print('File already exsits!')\n",
    "    # 获取文件大小\n",
    "    filesize = os.path.getsize(os.path.join(savepath, filename))\n",
    "    # 文件大小默认以Bytes计， 转换为Mb\n",
    "    print('File size = %.2f Mb' % (filesize/1024/1024))\n",
    "if __name__ == '__main__':\n",
    "    # 以下载cifar-10数据集为例\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    urls = df[4].values\n",
    "    names = df[0].values\n",
    "    a = list(range(25,12,-1))+[0,1,2]\n",
    "    for i in a:\n",
    "        print(i)\n",
    "        url = urls[i]\n",
    "        filename = str(i)+\".docx\"\n",
    "        try:\n",
    "            a = get_driver_url_content(url)\n",
    "            download(a,filename,savepath='./docx/')\n",
    "        except:\n",
    "            print(\"Skipping this, need special attention for \"+url)\n",
    "            print(names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有的methods/方程 没有main\n",
    "def xls_to_db_1(path):\n",
    "    \n",
    "    bigger_df = pd.DataFrame(columns = [\"table_index\",\"bad_type\",\"full_description\",\"publish_date\"])\n",
    "    for name in os.listdir(path)[0:3]:\n",
    "        books = xlrd.open_workbook(path+\"\\\\\"+name)\n",
    "        tb = books.sheets()[0]\n",
    "\n",
    "        index,bad_urls,bad_types,dates,descriptions=[],[],[],[],[]\n",
    "        table_index=0\n",
    "        pattern2 = re.compile(r\"([0-9]\\-[0-9])\")\n",
    "        pattern3 = re.compile(r\"(2017年.+|2016年.+)\")\n",
    "        for rown in range(tb.nrows):\n",
    "\n",
    "            pattern4 = re.compile(r\"[0-9].*\")\n",
    "            if bool(pattern4.match(tb.cell_value(rown,0))) == False:\n",
    "                continue\n",
    "\n",
    "            if url == None:\n",
    "                continue\n",
    "            try:\n",
    "                bad_type = \"类别\"+re.findall(pattern2,tb.cell_value(rown,3).replace(\" \",\"\"))[0]\n",
    "            except:\n",
    "                bad_type = \"类别\"+re.findall(pattern2,tb.cell_value(rown,2).replace(\" \",\"\"))[0]\n",
    "            bad_types.append(bad_type)         \n",
    "            bad_urls.append(url)\n",
    "            descriptions.append(tb.cell_value(rown,1).replace(\" \",\"\"))\n",
    "            try:\n",
    "                date = re.findall(pattern3,tb.cell_value(rown,4).replace(\" \",\"\"))[0].replace(\"年\",\"-\").replace(\"月\",\"\")\n",
    "            except:\n",
    "                try:\n",
    "                    date = re.findall(pattern3,tb.cell_value(rown,3).replace(\" \",\"\"))[0].replace(\"年\",\"-\").replace(\"月\",\"\")\n",
    "                except:\n",
    "                    date = \"2016-6\"\n",
    "            if len(date.split(\"-\")[1]) == 1:\n",
    "                date = list(date)\n",
    "                date.insert(5,\"0\")\n",
    "                date = \"\".join(date)\n",
    "            dates.append(date)\n",
    "\n",
    "\n",
    "\n",
    "        table_index = list(range(len(bad_urls)))    \n",
    "        df = pd.DataFrame({\"table_index\":table_index,\"bad_type\":bad_type,\"full_description\":descriptions,\"publish_date\":dates})\n",
    "        df = df.replace(dict_type)\n",
    "        bigger_df = pd.concat([bigger_df,df],axis = 0)\n",
    "    data_output_with_columns(bigger_df,\"bad_website\")\n",
    "    print(\"Data stored in db table name bad_website\")\n",
    "def weixin_to_db_2(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "\n",
    "    bs4_2 = BeautifulSoup(res, 'lxml')\n",
    "    ps = bs4_2.find_all(\"p\")\n",
    "    inames = []\n",
    "    dates = []\n",
    "    counter = 0\n",
    "    fees = [\"1.1万元\",\"51万元\",\"\",\"20万元\",\"2086.25万元\",\"500万元\"]\n",
    "    full_descriptions = []\n",
    "    for tag in ps:\n",
    "        match = re.search(r\"^(.*?支行|.*?分行|.*?分行|.*?分行)\", tag.text)\n",
    "        \n",
    "        if match:\n",
    "            \n",
    "            iname = re.findall(r\"^(.*?支行|.*?分行)\",tag.text)[0]\n",
    "            iname = re.split(r\"\\）|\\，\",iname)\n",
    "            if iname[-1] in inames:\n",
    "                continue\n",
    "            inames.append(iname[-1])\n",
    "            \n",
    "            \n",
    "            full_descriptions.append(tag.text)\n",
    "            try:\n",
    "                date = re.findall(r\"于(.*?月.*?日)\",tag.text)[0]\n",
    "            except:\n",
    "                date =  re.findall(r\"于(.*?月)\",tag.text)[0]\n",
    "            if date[-1] == \"月\":\n",
    "                date = date.replace(\"月\",\"\")\n",
    "            date = date.replace(\"年\",\"-\").replace(\"月\",\"-\").replace('日',\"\")\n",
    "            \n",
    "            \n",
    "            str_list = list(date)\n",
    "            if len(date.split(r\"-\")[1]) == 1:\n",
    "                str_list.insert(5,\"0\")\n",
    "                date = \"\".join(str_list)\n",
    "            dates.append(date)\n",
    "            counter+=1\n",
    "    xhs = list(range(counter))\n",
    "    df2 = pd.DataFrame({\"XH\":xhs,\"INAME\":inames,\"REG_DATE\":dates,\"FULL_DESCRIPTION\":full_descriptions,\"FEE\":fees})\n",
    "    df2[\"PUBLISH_DATE\"] = \"2020-07-24\"\n",
    "    df2[\"POST_BY\"] = \"中国银保监会\"\n",
    "    #df2[\"publish_date\"] = \"2020-07-04\"\n",
    "    data_output_with_columns(df2,\"YHWGSQSFAL\")\n",
    "    print(\"stored into DB table: YHWGSQSFAL\")\n",
    "\n",
    "    #display(df2)\n",
    "def weixin_to_db_1(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "\n",
    "    bs4_2 = BeautifulSoup(res, 'lxml')\n",
    "    ps = bs4_2.find_all(\"p\")\n",
    "    \n",
    "    pattern = re.compile(r\"公司$\")\n",
    "    inames = [tag.text for tag in ps if bool(pattern.search(tag.text)) == True]\n",
    "    xhs = list(range(len(inames)))\n",
    "    df = pd.DataFrame({\"XH\":xhs,\"INAME\":inames})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020-07-04\"\n",
    "\n",
    "    df[\"POST_BY\"] = \"中国银保监会\"\n",
    "    data_output_with_columns(df,\"zdwfwggdmd\")\n",
    "    print(\"stored into DB table: zdwfwggdmd\")\n",
    "def weixin_to_db_3(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "\n",
    "    bs4_2 = BeautifulSoup(res, 'lxml')\n",
    "    ps = bs4_2.find_all(\"p\")\n",
    "    \n",
    "    pattern = re.compile(r\"公司$\")\n",
    "    inames = [tag.text for tag in ps if bool(pattern.search(tag.text)) == True]\n",
    "    xhs = list(range(len(inames)))\n",
    "    df = pd.DataFrame({\"XH\":xhs,\"INAME\":inames})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020-07-10\"    \n",
    "    df[\"POST_BY\"] = \"人力资源和社会保障部\"\n",
    "    data_output_with_columns(df,\"rlzycxfw\")\n",
    "    print(\"stored into DB table: rlzycxfw\")\n",
    "def weixin_to_db_4(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "\n",
    "    bs4_2 = BeautifulSoup(res, 'lxml')\n",
    "    ps = bs4_2.find_all(\"p\")\n",
    "    \n",
    "    pattern = re.compile(r\"公司$\")\n",
    "    inames = [tag.text for tag in ps if bool(pattern.search(tag.text)) == True]\n",
    "    xhs = list(range(len(inames)))\n",
    "    df = pd.DataFrame({\"XH\":xhs,\"INAME\":inames})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020年7月29日\"\n",
    "\n",
    "    df[\"POST_BY\"] = \"中华人民共和国海事局\"\n",
    "    #data_output_with_columns(df,\"rlzycxfw\")\n",
    "    print(\"stored into DB table: rlzycxfw\")\n",
    "\n",
    "\n",
    "def pdf_to_db(url):\n",
    "    download(url,\"cwpzpt.pdf\",\"./pdfs\")\n",
    "    path = os.path.join(os.getcwd(),\"pdfs\")\n",
    "    path = os.path.join(path,\"cwpzpt.pdf\")\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        page=pdf.pages[0] #提取pdf第17页中的表格\n",
    "        tb = page.extract_tables()[0]\n",
    "        df = pd.DataFrame(columns = [\"xh\",\"pt_name\",\"bad_url\",\"wx_app_name\",\"jg_name\",\"area\"])\n",
    "        pattern1 = re.compile(r\"手机APP:(.+)微信公众号|手机APP：(.+)微信公众号\")\n",
    "        for i in range(len(tb[1:])):\n",
    "\n",
    "            df.loc[i] = [k.replace(\"\\n\",\"\") for k in tb[i+1]]\n",
    "\n",
    "        #display(df)\n",
    "        for page in pdf.pages[1:]:\n",
    "            #print(\"?\")\n",
    "            tb = page.extract_tables()[0]\n",
    "            curr_rows = len(df)\n",
    "\n",
    "            for i in range(len(tb)):\n",
    "\n",
    "                df.loc[curr_rows+i] = [k.replace(\"\\n\",\"\") for k in tb[i]]\n",
    "        data_output(df,\"cwpzpt\") \n",
    "        print(\"Stored into db table:cwpzpt!\")\n",
    "def data_output_with_columns(data,dataTable):\n",
    "    connection = db\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    columns = list(data.columns)\n",
    "    column_names = \",\".join(columns)\n",
    "    query = \"INSERT INTO \"+ dataTable+\"({})\".format(column_names)\n",
    "    query = query + \" VALUES ({})\"\n",
    "    aidx = list(range(1,len(columns)+1))\n",
    "    aidx = [':'+str(i) for i in aidx]\n",
    "    aname = ','.join(aidx)\n",
    "    dtHigh = data.shape[0]\n",
    "    dtWidth = data.shape[1]\n",
    "    creatVar = locals()\n",
    "    wholeData = []\n",
    "    for i in range(dtHigh):\n",
    "        value_list = []\n",
    "        for j in range(dtWidth):\n",
    "            value_list.append(\"{}\".format(str(data.iloc[i,j])))\n",
    "        wholeData.append(value_list)\n",
    "    cursor.executemany(query.format(aname),wholeData)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    return\n",
    "def get_request_content(url,landing_page = True,encoding='utf-8', timeout=3):\n",
    "    '''\n",
    "    使用浏览器获取动态内容 获取农名工拖欠工资的一个funtion\n",
    "    :param url:         网页url\n",
    "    :param encoding:    网页编码\n",
    "    :param timeout:     设置超时\n",
    "    :return:\n",
    "    '''\n",
    "    \n",
    "    import requests\n",
    "    from lxml import etree\n",
    "\n",
    "    headers =  {\n",
    "     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "    html = etree.HTML(res)\n",
    "    if landing_page == True:\n",
    "        link_path = html.xpath(\"//a[contains(text(), '信息表')]/@href\")[0]\n",
    "    else:\n",
    "        link_path = url\n",
    "    res2 = requests.get(link_path,headers=headers).content\n",
    "    bs4_2 = BeautifulSoup(res2, 'lxml')\n",
    "    trs = bs4_2.find(attrs={'id':'insMainConTxt'}).find(\"tbody\").find_all(\"tr\")\n",
    "    xhs = []\n",
    "    DXMCs = []\n",
    "    ZJLX = []\n",
    "    ZJHM = []\n",
    "    FRDB = []\n",
    "    FRDBZJHM = []\n",
    "    LYSY = []\n",
    "    LRRQ = []\n",
    "    \n",
    "    for tr in trs[1:]:\n",
    "        \n",
    "        tds = tr.find_all(\"td\")\n",
    "        if len(tds) ==2:\n",
    "            name = tds[0].text.replace(\"\\n\",\"\")\n",
    "            id_num = tds[1].text.replace(\"\\n\",\"\")\n",
    "            FRDB[-1] = [FRDB[-1],name]\n",
    "            FRDBZJHM[-1] = [FRDBZJHM[-1],id_num]\n",
    "            continue\n",
    "            \n",
    "\n",
    "            \n",
    "        xhs.append(tds[0].text.replace(\"\\n\",\"\"))\n",
    "        \n",
    "        DXMCs.append(tds[2].text.replace(\"\\n\",\"\"))\n",
    "        #ZJLX.append(tds[3].text)\n",
    "        ZJHM.append(tds[3].text.replace(\"\\n\",\"\"))\n",
    "        FRDB.append(tds[4].text.replace(\"\\n\",\"\"))\n",
    "        FRDBZJHM.append(tds[5].text.replace(\"\\n\",\"\"))\n",
    "        LYSY.append(tds[6].text.replace(\"\\n\",\"\"))\n",
    "        A = list(tds[7].text.replace(\"\\n\",\"\"))\n",
    "        A.insert(4,\"-\")\n",
    "        A.insert(7,\"-\")\n",
    "        LRRQ.append(\"\".join(A))\n",
    "        \n",
    "    df = pd.DataFrame({\"XH\":xhs,\"DXMC\":DXMCs,\"ZJHM\":ZJHM,\"FRDB\":FRDB,\"FRDBZJHM\":FRDBZJHM,\"LYSY\":LYSY,\"LRRQ\":LRRQ})\n",
    "    df[\"ZJLX\"] = \"统一社会信用代码\"\n",
    "    df[\"FRDBZJLX\"] = \"身份证号\"\n",
    "    \n",
    "    #db = oracle.connect('tenant01/123456@127.0.0.1:1521/orcl')\n",
    "    \n",
    "    data_output_with_columns(df,\"NMG\")\n",
    "    #db.close()\n",
    "    print(\"Successfully put in db table NMG:   \"+url)\n",
    "    #return df\n",
    "def weixin_to_db_4(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    res = requests.get(url, headers=headers).content\n",
    "\n",
    "    bs4_2 = BeautifulSoup(res, 'lxml')\n",
    "    ps = bs4_2.find_all(\"p\")\n",
    "    types = []\n",
    "    pattern = re.compile(r\"公司$\")\n",
    "    pattern2 = re.compile(r\"^[一二三]\")\n",
    "    inames = []\n",
    "    for tag in ps:\n",
    "        if bool(pattern2.search(tag.text)) == True:\n",
    "            ty = tag.text.split(\"、\")[1]\n",
    "        \n",
    "        if bool(pattern.search(tag.text)) == True:\n",
    "            inames.append(tag.text)\n",
    "            types.append(ty)\n",
    "    xhs = list(range(len(inames)))\n",
    "    df = pd.DataFrame({\"XH\":xhs,\"INAME\":inames,\"in_out\":types})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020-07-29\"\n",
    "    \n",
    "    \n",
    "    df[\"POST_BY\"] = \"中华人民共和国海事局\"\n",
    "    types_dict = {}\n",
    "    types_dict[\"拟新评选安全诚信航运公司名单\"] = \"new_in\"\n",
    "    types_dict[\"拟通过年度评价的安全诚信航运公司名单\"] = \"keep_in\"\n",
    "    types_dict[\"拟撤销安全诚信航运公司资格的名单\"] = \"out\"\n",
    "    df = df.replace(types_dict)\n",
    "    data_output_with_columns(df,\"aqcxhkgshxmd\")\n",
    "    print(\"stored into DB table: aqcxhkgshxmd\")\n",
    "def xls_to_db_medicine(path):\n",
    "    \n",
    "    #bigger_df = pd.DataFrame(columns = [\"table_index\",\"bad_type\",\"full_description\",\"publish_date\"])\n",
    "    for name in os.listdir(path):\n",
    "        books = xlrd.open_workbook(path+\"\\\\\"+name)\n",
    "        tb = books.sheets()[0]\n",
    "\n",
    "        type_names,size,product_name,batch_number,iname,gist_code,result,reason,gist_unit,remark = [],[],[],[],[],[],[],[],[],[]\n",
    "        for rown in range(tb.nrows)[2:]:\n",
    "            type_names.append(tb.cell_value(rown,0))\n",
    "            size.append(tb.cell_value(rown,1))\n",
    "            product_name.append(tb.cell_value(rown,2))\n",
    "            batch_number.append(tb.cell_value(rown,3))\n",
    "            iname.append(tb.cell_value(rown,4))\n",
    "            gist_code.append(tb.cell_value(rown,5))\n",
    "            result.append(tb.cell_value(rown,6))\n",
    "            reason.append(tb.cell_value(rown,7))\n",
    "            gist_unit.append(tb.cell_value(rown,8))\n",
    "            remark.append(tb.cell_value(rown,9))\n",
    "    df=pd.DataFrame({\"TYPE_NAME\":type_names,\"MEDICINE_SIZE\":size,\"PRODUCT_NAME\":product_name,\"BATCH_NUMBER\":batch_number,\n",
    "                     \"INAME\":iname,\n",
    "                     \"GIST_CODE\":gist_code,\n",
    "                     'TEST_RESULT':result,'REASON':reason,'GIST_UNIT':gist_unit,'REMARK':remark})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020-07-04\"\n",
    "    df[\"POST_BY\"] = \"国家药品监督管理局\"\n",
    "    df = df.replace(\"/\",\"\")\n",
    "    data_output_with_columns(df,\"medicine_blacklist\")\n",
    "    print(\"Stored into db table: medicine_blacklist\")\n",
    "def weixin_to_db_5(url):\n",
    "    headers =  {\n",
    "         'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    req = request.Request(url, headers=headers)\n",
    "    response = request.urlopen(req)\n",
    "    #bs4_2 = BeautifulSoup(response.text)\n",
    "    #ps = bs4_2.find_all(\"p\")\n",
    "    data = response.read()  # 读取响应结果\n",
    "    data = str(data, encoding='utf-8')\n",
    "    data = data.split(\"</html>\")[1]\n",
    "    bs4_ = BeautifulSoup(data)\n",
    "    ps = bs4_.find_all(\"p\")\n",
    "    pattern = re.compile(r\"统一社会信用代码|信用编号\")\n",
    "    pattern2 = re.compile(r\"并将相关情况记入其诚信档案\")\n",
    "    pattern3 = re.compile(r\"统一社会信用代码\")\n",
    "    inames = []\n",
    "    descriptions=[]\n",
    "    card_nums = []\n",
    "    reg_dates = []\n",
    "    card_types = []\n",
    "    for tag in ps:\n",
    "        \n",
    "        if bool(pattern.search(tag.text)) == True:\n",
    "            iname = re.findall(r\"、(.+)（\",tag.text)[0]\n",
    "            card_num = re.findall(r\"：(.+)）\",tag.text)[0]\n",
    "            card_type = \"统一社会信用代码\"\n",
    "            if bool(pattern3.search(tag.text)) == False:\n",
    "                card_type = \"信用编号\"\n",
    "            card_types.append(card_type)\n",
    "            inames.append(iname)\n",
    "            card_nums.append(card_num)\n",
    "        if bool(pattern2.search(tag.text)) == True:\n",
    "            reg_date = re.findall(r\"^(.*?日)\",tag.text)[0].replace(\"年\",\"-\").replace(\"月\",\"-\").replace(\"日\",\"\")\n",
    "            if len(reg_date.split(\"-\")[1]) == 1:\n",
    "                reg_date = list(reg_date)\n",
    "                reg_date.insert(5,\"0\")\n",
    "                reg_date = \"\".join(reg_date)\n",
    "            reg_dates.append(reg_date)\n",
    "            descriptions.append(tag.text)\n",
    "    \n",
    "    xhs = list(range(len(inames)))\n",
    "    df = pd.DataFrame({\"XH\":xhs,\"INAME\":inames,\"FULL_DESCRIPTION\":descriptions,\n",
    "                       \"CARD_TYPE\":card_types,\"CARD_NUM\":card_nums,\"REG_DATE\":reg_dates})\n",
    "    df[\"PUBLISH_DATE\"] = \"2020-07-21\"\n",
    "    df[\"POST_BY\"] = \"生态环境部\"\n",
    "    #df[\"POST_BY\"] = \"中国银保监会\"\n",
    "    #data_output_with_columns(df,\"zdwfwggdmd\")\n",
    "    #print(\"stored into DB table:\n",
    "    data_output_with_columns(df,\"environment_blacklist\")\n",
    "    print(\"Stored in DB table: environment_blacklist\")\n",
    "\n",
    "\n",
    "def get_web_content(url):\n",
    "    headers =  {\n",
    "     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    bs4_2 = BeautifulSoup(res.text, 'lxml')\n",
    "    trs = bs4_2.find_all(\"table\", \"MsoNormalTable\")[0].find_all(\"tr\")[12:63]\n",
    "    pattern1 = re.compile(r\"\\d\")\n",
    "    pattern2 = re.compile(r\".+(http:/.+)\")\n",
    "\n",
    "    xhs = []\n",
    "    full_descriptions = []\n",
    "    bad_types = []\n",
    "    reg_dates = []\n",
    "    for tr in trs:\n",
    "        \n",
    "        xh = tr.find_all(\"td\")[0].text.replace(\"\\n\",\"\")\n",
    "        if bool(pattern1.search(xh)) == False:\n",
    "            continue\n",
    "            \n",
    "        full_description = tr.find_all(\"td\")[1].text.replace(\"\\n\",\"\")\n",
    "        \n",
    "        \n",
    "        full_descriptions.append(full_description)\n",
    "        \n",
    "        \n",
    "        \n",
    "        bad_type = dict_type[tr.find_all(\"td\")[2].text.replace(\"\\n\",\"\")]\n",
    "        bad_types.append(bad_type)\n",
    "        xhs.append(xh)\n",
    "        \n",
    "        reg_date = tr.find_all(\"td\")[3].text.replace(\"\\n\",\"\")\n",
    "        reg_date = reg_date.replace(\"年\",\"-\").replace(\"月\",\"\")\n",
    "        if len(reg_date.split(\"-\")[1]) == 1:\n",
    "            reg_date = list(reg_date)\n",
    "            reg_date.insert(5,\"0\")\n",
    "            reg_date = \"\".join(reg_date)\n",
    "        reg_dates.append(reg_date)\n",
    "    df=pd.DataFrame({\"TABLE_INDEX\":xhs,\"BAD_TYPE\":bad_types,\"FULL_DESCRIPTION\":full_descriptions,\"PUBLISH_DATE\" : reg_dates})\n",
    "    \n",
    "    data_output_with_columns(df,\"bad_website\")\n",
    "    print(\"Stored into db table:bad_website\")\n",
    "def xls_to_db_2(url,name):\n",
    "    download(url,name)\n",
    "    XH = []\n",
    "    DXMC = []\n",
    "    ZJLX = []\n",
    "    ZJHM = []\n",
    "    FRDB = []\n",
    "    FRDBZJHM = []\n",
    "    LYSY = []\n",
    "    LRRQ = []\n",
    "    SJJE = []\n",
    "    RDBM = []\n",
    "    bigger_df = pd.DataFrame(columns = [])\n",
    "    \n",
    "    books = xlrd.open_workbook(name)\n",
    "    tb = books.sheets()[0]\n",
    "\n",
    "    table_index=0\n",
    "    pattern2 = re.compile(r\"([0-9]\\-[0-9])\")\n",
    "    pattern3 = re.compile(r\"(2017年.+|2016年.+)\")\n",
    "    for rown in range(2,tb.nrows):\n",
    "        if tb.cell_value(rown,0) == \"\":\n",
    "            if type(FRDB[-1])!=list:\n",
    "                FRDBZJHM[-1] = [FRDBZJHM[-1]]\n",
    "                FRDB[-1] = [FRDB[-1]]\n",
    "            FRDB[-1].append(tb.cell_value(rown,4))\n",
    "            FRDBZJHM[-1].append(tb.cell_value(rown,5))\n",
    "            continue\n",
    "        XH.append(tb.cell_value(rown,0))\n",
    "        DXMC.append(tb.cell_value(rown,2))\n",
    "        ZJHM.append(tb.cell_value(rown,3))\n",
    "        \n",
    "        ZJLX.append(\"统一社会信用代码\")\n",
    "        FRDB.append(tb.cell_value(rown,4))\n",
    "        FRDBZJHM.append(tb.cell_value(rown,5))\n",
    "        LYSY.append(tb.cell_value(rown,6))\n",
    "        LRRQ.append(\"2019-01-23\")\n",
    "        SJJE.append(tb.cell_value(rown,7))\n",
    "        RDBM.append(tb.cell_value(rown,8))\n",
    "\n",
    "    df = pd.DataFrame({\"XH\":XH,\"DXMC\":DXMC,\"ZJHM\":ZJHM,\"FRDB\":FRDB,\n",
    "                       \"FRDBZJHM\":FRDBZJHM,\"LYSY\":LYSY,\"LRRQ\":LRRQ\n",
    "                      ,\"RDBM\":RDBM,\"SJJE\":SJJE})\n",
    "    df[\"FRDBZJLX\"] = \"身份证号\"\n",
    "    \n",
    "    data_output_with_columns(df,\"nmg\")\n",
    "    print(\"Data stored in db table name NMG\")\n",
    "\n",
    "#print(len(doc.tables[0].rows))\n",
    "def doc_to_df(doc):\n",
    "    pattern2 = re.compile(r\"博客\")\n",
    "    pattern = re.compile(r\"类别|表现形式|[1-9]+|序号\")\n",
    "    row_nums = []\n",
    "    row_type = []\n",
    "    tb = doc.tables[0]\n",
    "    for row_num in range(len(tb.rows)):\n",
    "        row_title = tb.rows[row_num].cells[0].text\n",
    "        if bool(pattern.search(row_title))==False:\n",
    "            row_nums.append(row_num+1)\n",
    "            row_type.append(row_title)\n",
    "    \n",
    "    row_nums.append(len(tb.rows))\n",
    "    for i in range(len(row_nums)-1):\n",
    "        find_df(tb,row_nums[i],row_nums[i+1]-1)\n",
    "\n",
    "def data_output(data,dataTable):\n",
    "    connection = db\n",
    "    cursor = connection.cursor()\n",
    "    query = \"INSERT INTO \"+ dataTable + \" VALUES ({})\"\n",
    "    columns = list(data.columns)\n",
    "    aidx = list(range(1,len(columns)+1))\n",
    "    aidx = [':'+str(i) for i in aidx]\n",
    "    aname = ','.join(aidx)\n",
    "    dtHigh = data.shape[0]\n",
    "    dtWidth = data.shape[1]\n",
    "    creatVar = locals()\n",
    "    wholeData = []\n",
    "    for i in range(dtHigh):\n",
    "        value_list = []\n",
    "        for j in range(dtWidth):\n",
    "            value_list.append(\"{}\".format(str(data.iloc[i,j])))\n",
    "        wholeData.append(value_list)\n",
    "    cursor.executemany(query.format(aname),wholeData)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    return\n",
    "def find_df(tb,row1,row2):\n",
    "    index,bad_urls,bad_type,date,descriptions=[],[],[],[],[]\n",
    "    table_index=0\n",
    "    pattern2 = re.compile(r\"(.+)http:/.+\")\n",
    "    pattern3 = re.compile(r\"\\“(.+)”\")\n",
    "    pattern4 = re.compile(r\"\\：(.+)http\")\n",
    "    \n",
    "    for row in tb.rows[row1:row2]:\n",
    "        text= \"\"\n",
    "        for p in row.cells[2].paragraphs:##如果cell中有多段，即有回车符\n",
    "            text+=p.text\n",
    "        \n",
    "        bad_type.append(row.cells[3].text)         \n",
    "        descriptions.append(row.cells[1].text)\n",
    "        date.append(row.cells[4].text)\n",
    "        index.append(row.cells[0].text)#这行也可以放在else中\n",
    "\n",
    "    df=pd.DataFrame({\"table_index\":index,'publish_date':date,\"bad_type\":bad_type,\"full_description\":descriptions})\n",
    "    df = df.replace(dict_type)\n",
    "    data_output_with_columns(df,\"bad_website\")\n",
    "\n",
    "def get_stock_df(doc):\n",
    "    \n",
    "    tb = doc.tables[0]\n",
    "    index,stock_num,iname,start,end = [],[],[],[],[]\n",
    "    for row in tb.rows[1:]:\n",
    "        \n",
    "        cells = row.cells\n",
    "        index.append(cells[0].text)\n",
    "        stock_num.append(cells[1].text)\n",
    "        iname.append(cells[2].text)\n",
    "        start.append(cells[3].text.replace(\"年\",\"-\").replace(\"月\",\"-\").replace(\"日\",\"-\"))\n",
    "        end.append(cells[4].text.replace(\"年\",\"-\").replace(\"月\",\"-\").replace(\"日\",\"-\"))\n",
    "    df = pd.DataFrame({\"row_index\":index,\"stock_name\":stock_num,\"iname\":iname,\"start_date\":start,\"end_date\":end})\n",
    "    data_output(df,\"stock_peishou\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username(必须大写）:   \n",
      "Password:   \n",
      "数据库IP(默认127.0.0.1):   \n",
      "Port(默认1521):    \n",
      "服务名/数据库(默认orcl):    \n",
      "stored into DB table: zdwfwggdmd\n",
      "stored into DB table: YHWGSQSFAL\n",
      "stored into DB table: rlzycxfw\n",
      "stored into DB table: aqcxhkgshxmd\n",
      "Stored into db table: medicine_blacklist\n",
      "Stored in DB table: environment_blacklist\n",
      "Stored into db table:bad_website\n",
      "File already exsits!\n",
      "File size = 0.03 Mb\n",
      "Data stored in db table name NMG\n",
      "农名工:  http://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/202007/t20200730_381374.html\n",
      "Successfully put in db table NMG:   http://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/202007/t20200730_381374.html\n",
      "农名工:  http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodongtai/202005/t20200512_368309.html\n",
      "Successfully put in db table NMG:   http://www.mohrss.gov.cn/ldjcj/LDJCJgongzuodongtai/202005/t20200512_368309.html\n",
      "File already exsits!\n",
      "File size = 0.20 Mb\n",
      "Stored into db table:cwpzpt!\n",
      "Data stored in db table name bad_website\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\0.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\1.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\2.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\14.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\15.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\16.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\17.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\18.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\19.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\20.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\21.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\22.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\23.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\24.docx\n",
      "C:\\Users\\Wendy W\\Documents\\GitHub\\crawl_blacklist\\docx\\25.docx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#运行 main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = input(\"Username(必须大写）:   \")\n",
    "    password = input(\"Password:   \")\n",
    "    database_ip = input(\"数据库IP(默认127.0.0.1):   \")\n",
    "    port = input(\"Port(默认1521):    \")\n",
    "    database = input(\"服务名/数据库(默认orcl):    \")\n",
    "    string = username+\"/\"+password+\"@\"+database_ip+\":\"+\"port\"+\"/\"+database\n",
    "    try:\n",
    "        db = oracle.connet(string)\n",
    "    except:\n",
    "        db = oracle.connect('tenant01/123456@127.0.0.1:1521/orcl')\n",
    "    dict_df = pd.read_csv(\"type_dict.csv\")\n",
    "    dict_type = dict(zip(dict_df[\"type\"],dict_df[\"description\"]))\n",
    "    url1 = \"https://mp.weixin.qq.com/s/VMr3X1g_psMkcze_6Vlj6Q\"    \n",
    "\n",
    "    weixin_to_db_1(url1)\n",
    "    url2 = \"https://mp.weixin.qq.com/s/qNsQmC4fN9S1mbsAA3OdiQ\"    \n",
    "    weixin_to_db_2(url2)\n",
    "    url3 = \"https://mp.weixin.qq.com/s/ODgW4cUU6QZvngW8VpIktQ\"\n",
    "    weixin_to_db_3(url3)\n",
    "    url4 = \"https://www.msa.gov.cn/page/article.do?articleId=423B298D-156B-42FB-A02C-B73E9DD349A9\"\n",
    "    weixin_to_db_4(url4)\n",
    "    path = os.path.join(os.getcwd(),\"medicine\")\n",
    "    xls_to_db_medicine(path)\n",
    "    url5 = \"https://m.credit100.com/xhxy/c/2020-07-21/624567.shtml?title=%E6%96%B0%E5%8D%8E%E4%BF%A1%E7%94%A8-%E4%BF%A1%E7%94%A8%E5%A4%B4%E6%9D%A1&from=groupmessage&isappinstalled=0\"\n",
    "     \n",
    "    weixin_to_db_5(url5)\n",
    "    \n",
    "    url6 = \"http://www.csrc.gov.cn/guangxi/xxfw/tjyg/201810/t20181017_345402.htm\"\n",
    "    get_web_content(url6)           \n",
    "    a = [0,1,2]+list(range(14,26))\n",
    "    name = \"nmg_last.xls\"\n",
    "    url = \"http://www.mohrss.gov.cn/SYrlzyhshbzb/dongtaixinwen/buneiyaowen/201901/W020190123533804955846.xls\"\n",
    "    xls_to_db_2(url,name)\n",
    "    b = [df[4].values[12],df[4].values[28]]\n",
    "    for url in b:\n",
    "        print(\"农名工:  \"+url)\n",
    "        try:\n",
    "            get_request_content(url)\n",
    "        except:\n",
    "            get_request_content(url,False)\n",
    "    doc_path = os.path.join(os.getcwd(), \"docx\\\\\"+\"13.docx\")\n",
    "    doc = docx.Document(doc_path)\n",
    "    get_stock_df(doc)\n",
    "    pdf_to_db(\"http://www.csrc.gov.cn/pub/newsite/zjhxwfb/xwdd/202007/P020200708631496562495.pdf\")\n",
    "    \n",
    "    \n",
    "    path = os.path.join(os.getcwd(),\"img_output_forms\")\n",
    "    xls_to_db_1(path)\n",
    "    for i in a:\n",
    "        file = str(i)+\".docx\"\n",
    "        doc_path = os.path.join(os.getcwd(), \"docx\\\\\"+file)\n",
    "        print(doc_path)\n",
    "        document = docx.Document(doc_path)\n",
    "        doc_to_df(document)\n",
    "    \n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
